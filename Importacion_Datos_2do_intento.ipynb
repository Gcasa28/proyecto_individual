{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de librerias a utilizar\n",
    "import pandas as pd\n",
    "import json\n",
    "import locale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yanina Lucia\\AppData\\Local\\Temp\\ipykernel_10024\\3851153582.py:2: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"movies_dataset_2do_intento.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Convierto mi archivo csv en un dataframe de pandas\n",
    "df = pd.read_csv(\"movies_dataset_2do_intento.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quito las columnas las cuales no tomaremos en cuenta para trabajar con ellas\n",
    "df = df.drop([\"adult\", \"homepage\", \"imdb_id\", \"original_title\", \"poster_path\", \"runtime\", \"video\"], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo esta funcion para desanidar las columnas que lo requieran\n",
    "def safe_json_loads(s):\n",
    "    try:\n",
    "        return json.loads(s) #Cargo datos Json\n",
    "    except:\n",
    "        return {} #Relleno con {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yanina Lucia\\AppData\\Local\\Temp\\ipykernel_10024\\1957826267.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"belongs_to_collection\"] = df[\"belongs_to_collection\"].str.replace('\"backdrop_path\": None}', '\"backdrop_path\": \"\"}')\n"
     ]
    }
   ],
   "source": [
    "#Cambio las comillas simples por dobles\n",
    "df[\"belongs_to_collection\"] = df[\"belongs_to_collection\"].str.replace(\"'\",'\"')\n",
    "#Cambio los \"None\" por \"\"\n",
    "df[\"belongs_to_collection\"] = df[\"belongs_to_collection\"].str.replace('\"backdrop_path\": None}', '\"backdrop_path\": \"\"}')\n",
    "#Cambio \"NaN\" por \"{}\"\"\n",
    "df[\"belongs_to_collection\"] = df[\"belongs_to_collection\"].fillna('{}')\n",
    "#Normalizo datos Json en una tabla plana y los cargo a la base de datos\n",
    "df_belongs = pd.json_normalize(df[\"belongs_to_collection\"].apply(safe_json_loads))\n",
    "#dropeo las columnas que no necesito\n",
    "df_belongs = df_belongs.drop([\"id\", \"poster_path\", \"backdrop_path\"], axis = 1)\n",
    "#Sustituyo datos\n",
    "df[\"belongs_to_collection\"] = df_belongs\n",
    "#Cambiamos \"NaN\" por \"No collection\"\n",
    "df[\"belongs_to_collection\"] = df[\"belongs_to_collection\"].fillna(\"No collection\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HASTA ACA YA LA COLUMNA \"BELONG_TO_COLLECTION\" ESTA DESANIDADA, AHORA IRE CON LA DE \"GENRES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos las comillas simples por dobles\n",
    "df['genres'] = df['genres'].str.replace(\"'\", '\"')\n",
    "# Cambiamos los NaN por {}\n",
    "df['genres'] = df['genres'].fillna('{}')\n",
    "# Normalizamos los datos JSON (listas de diccionarios) en una tabla plana y los cargamos\n",
    "df_genres = pd.json_normalize(df['genres'].apply(safe_json_loads))  \n",
    "#Creo un lista que voy a llenar con los datos de cada columna\n",
    "dfs_genres = []\n",
    "# Recorremos las columnas de los datos normalizados que todavía tienen formato JSON (diccionarios)\n",
    "for i in range (df_genres.shape[1]):  \n",
    "    df_genres[[i]] = df_genres[[i]].astype(str)\n",
    "    df_genres[i] = df_genres[i].str.replace(\"'\", '\"')\n",
    "    df_genres[i] = df_genres[i].fillna('{}')\n",
    "    globals()['df_genres{}'.format(i)] = pd.json_normalize(df_genres[i].apply(safe_json_loads)).astype(str)\n",
    "    dfs_genres.append(globals()['df_genres{}'.format(i)])  # Agregar cada DataFrame resultante a la lista\n",
    "# Concatena todos los DataFrames de la lista en uno solo\n",
    "df_genres_resul = pd.concat(dfs_genres, axis=1)\n",
    "#Tomo todas las columnas para trabajar con ellas\n",
    "df_nuevo = df_genres_resul.iloc[:, 0:16]\n",
    "#dropeo las columnas que no me sirven\n",
    "df_nuevo = df_nuevo.drop(\"id\", axis= 1)\n",
    "#Ingreso los datos en cada columna\n",
    "df_nuevo['nueva_col'] = df_nuevo.apply(lambda row: [x for x in row[['name']].values if not pd.isna(x)], axis=1)\n",
    "#Filtro las filas y elimino los \"nan\"y los ingreso en una nueva columna\n",
    "df_nuevo['nueva_col'] = [list(filter(lambda x: x!='nan', lst)) for lst in df_nuevo['nueva_col']]\n",
    "#dropeo las columnas que ya no me sirven\n",
    "df_nuevo.drop([\"name\"], axis = 1)\n",
    "#Sustituyo la columna\n",
    "df['genres'] = df_nuevo[\"nueva_col\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HASTA ACA YA LA COLUMNA \"GENRES\" ESTA DESANIDADA, AHORA IRE CON LA DE \"PRODUCTION_COMPANIES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos las comillas simples por dobles\n",
    "df['production_companies'] = df['production_companies'].str.replace(\"'\", '\"')\n",
    "# Cambiamos los NaN por {}\n",
    "df['production_companies'] = df['production_companies'].fillna('{}')\n",
    "# Normalizamos los datos JSON (listas de diccionarios) en una tabla plana y los cargamos\n",
    "df_production = pd.json_normalize(df['production_companies'].apply(safe_json_loads))\n",
    "#Creo un lista que voy a llenar con los datos de cada columna\n",
    "dfs_companie = []\n",
    "# Recorremos las columnas de los datos normalizados que todavía tienen formato JSON (diccionarios)\n",
    "for i in range (0, df_production.shape[1]):\n",
    "    df_production[[i]] = df_production[[i]].astype(str)\n",
    "    df_production[i] = df_production[i].str.replace(\"'\", '\"')\n",
    "    df_production[i] = df_production[i].fillna('{}')\n",
    "    globals()['df_production{}'.format(i)] = pd.json_normalize(df_production[i].apply(safe_json_loads)).astype(str)\n",
    "    dfs_companie.append(globals()['df_production{}'.format(i)])  # Agregar cada DataFrame resultante a la lista\n",
    "# Concatenar todos los DataFrames de la lista en uno solo\n",
    "df_production_resul = pd.concat(dfs_companie, axis=1)\n",
    "#Tomo todas las columnas para trabajar con ellas\n",
    "df_nuevo = df_production_resul.iloc[:, 0:52]\n",
    "#dropeo las columnas que no me sirven\n",
    "df_nuevo = df_nuevo.drop(\"id\", axis= 1)\n",
    "#Ingreso los datos en cada columna\n",
    "df_nuevo['nueva_col'] = df_nuevo.apply(lambda row: [x for x in row[['name']].values if not pd.isna(x)], axis=1)\n",
    "#Filtro las filas y elimino los \"nan\"y los ingreso en una nueva columna\n",
    "df_nuevo['nueva_col'] = [list(filter(lambda x: x!='nan', lst)) for lst in df_nuevo['nueva_col']]\n",
    "#dropeo las columnas que ya no me sirven\n",
    "df_nuevo.drop([\"name\"], axis = 1)\n",
    "#Sustituyo la columna\n",
    "df[\"production_companies\"] = df_nuevo[\"nueva_col\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HASTA ACA YA LA COLUMNA \"PRODUCTION_COMPANIES\" ESTA DESANIDADA, AHORA IRE CON LA DE \"PRODUCTION_COUNTRIES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos las comillas simples por dobles\n",
    "df['production_countries'] = df['production_countries'].str.replace(\"'\", '\"')\n",
    "# Cambiamos los NaN por {}\n",
    "df['production_countries'] = df['production_countries'].fillna('{}')\n",
    "# Normalizamos los datos JSON (listas de diccionarios) en una tabla plana y los cargamos\n",
    "df_countries = pd.json_normalize(df['production_countries'].apply(safe_json_loads))\n",
    "#Creo un lista que voy a llenar con los datos de cada columna\n",
    "dfs_country = []\n",
    "# Recorremos las columnas de los datos normalizados que todavía tienen formato JSON (diccionarios)\n",
    "for i in range (0, df_countries.shape[1]):\n",
    "    df_countries[[i]] = df_countries[[i]].astype(str)\n",
    "    df_countries[i] = df_countries[i].str.replace(\"'\", '\"')\n",
    "    df_countries[i] = df_countries[i].fillna('{}')\n",
    "    globals()['df_countries{}'.format(i)] = pd.json_normalize(df_countries[i].apply(safe_json_loads)).astype(str)\n",
    "    dfs_country.append(globals()['df_countries{}'.format(i)])  # Agregar cada DataFrame resultante a la lista\n",
    "# Concatenar todos los DataFrames de la lista en uno solo\n",
    "df_countries_resul = pd.concat(dfs_country, axis=1)\n",
    "#Tomo todas las columnas para trabajar con ellas\n",
    "df_nuevo = df_countries_resul.iloc[:, 0:50]\n",
    "#dropeo las columnas que no me sirven\n",
    "df_nuevo = df_nuevo.drop([\"iso_3166_1\"], axis = 1)\n",
    "#Ingreso los datos en cada columna\n",
    "df_nuevo['nueva_col'] = df_nuevo.apply(lambda row: [x for x in row[[\"name\"]].values if not pd.isna(x)], axis=1)\n",
    "#Filtro las filas y elimino los \"nan\"y los ingreso en una nueva columna\n",
    "df_nuevo['nueva_col'] = [list(filter(lambda x: x!='nan', lst)) for lst in df_nuevo['nueva_col']]\n",
    "#dropeo las columnas que ya no me sirven\n",
    "df_nuevo = df_nuevo.drop([\"name\"], axis = 1)\n",
    "#Sustituyo la columna\n",
    "df[\"production_countries\"] = df_nuevo[\"nueva_col\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HASTA ACA YA LA COLUMNA \"PRODUCTION_COUNTRIES\" ESTA DESANIDADA, AHORA IRE CON LA DE \"SPOKEN_LENGUAGES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desanidamos los datos de la columna 'spoken_languages'\n",
    "df['spoken_languages'] = df['spoken_languages'].str.replace(\"'\", '\"')  # Sustituimos las comillas simples por dobles\n",
    "df['spoken_languages'] = df['spoken_languages'].fillna('{}')   # Rellenamos NaN con {}\n",
    "df_spoken_languages = pd.json_normalize(df['spoken_languages'].apply(safe_json_loads))  # Normalizamos los datos JSON (lista de diccionarios) en una tabla plana y los cargamos\n",
    "dfs_languages = []\n",
    "\n",
    "for i in range (df_spoken_languages.shape[1]):  # Recorremos las columnas de los datos normalizados que todavía tienen formato JSON (diccionarios)\n",
    "    df_spoken_languages[[i]] = df_spoken_languages[[i]].astype(str)  # Indicamos que los datos son tipo string\n",
    "    df_spoken_languages[i] = df_spoken_languages[i].str.replace(\"'\", '\"')  # Sustituimos las comillas simples por dobles\n",
    "    df_spoken_languages[i] = df_spoken_languages[i].fillna('{}')   # Rellenamos NaN con {}\n",
    "    # Normalizamos los datos JSON en una tabla plana y los cargamos\n",
    "    # Copiamos los valores no nulos en la lista de cada fila del DataFrame\n",
    "    globals()['df_spoken_languages{}'.format(i)] = pd.json_normalize(df_spoken_languages[i].apply(safe_json_loads)).astype(str)\n",
    "    dfs_languages.append(globals()['df_spoken_languages{}'.format(i)])  # Agregamos cada DataFrame resultante a la lista\n",
    "\n",
    "df_languages_resul = pd.concat(dfs_languages, axis=1)  # Concatena todos los DataFrames de la lista en uno solo\n",
    "df_nuevo = df_languages_resul.iloc[:, 0:26]  # Localizamos todas las filas para las 26 columnas del DataFrame df_languages_resul y las guardamos en df_nuevo\n",
    "df_nuevo = df_nuevo.drop(\"iso_639_1\", axis= 1)  # Dropeamos las columnas id iso_639_1\n",
    "# Copiamos los valores no nulos en la lista de cada fila del DataFrame\n",
    "df_nuevo['nueva_col'] = df_nuevo.apply(lambda row: [x for x in row[['name']].values if not pd.isna(x)], axis=1)\n",
    "df_nuevo['nueva_col'] = [list(filter(lambda x: x!='nan', lst)) for lst in df_nuevo['nueva_col']]\n",
    "df_nuevo.drop([\"name\"], axis = 1)  # Dropeamos las columnas name\n",
    "df[\"spoken_languages\"] = df_nuevo[\"nueva_col\"]  # Sustituimos los valores de df[\"spoken_languages\"] por los valores de df_nuevo[\"nueva_col\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeo si hay nulos o series con NaN en ellas y reemplazamos\n",
    "df[\"revenue\"].isnull().sum()\n",
    "df[\"revenue\"].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropeo las series con NaN\n",
    "df = df.dropna(subset=[\"release_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es_ES'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importo locale para cambiar las fechas y horarios a español\n",
    "locale.setlocale(locale.LC_TIME, \"es_ES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrijo el el formato de la fecha\n",
    "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo 3 nuevas columnas para trabajar mas facil con ellas\n",
    "df[\"release_year\"] = df[\"release_date\"].dt.strftime('%Y')\n",
    "\n",
    "df[\"release_month\"] = df[\"release_date\"].dt.strftime('%B')\n",
    "\n",
    "df[\"release_day\"] = df[\"release_date\"].dt.strftime('%A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"release_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo el retorno de las peliculas\n",
    "df[\"budget\"] = df[\"budget\"].astype(float)\n",
    "df[\"return\"] = df[\"revenue\"] / df[\"budget\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reemplazo donde me haya dado infinito o NaN por \"0\"\n",
    "df[\"return\"] = df[\"return\"].replace([np.inf, 0])\n",
    "df[\"return\"] = df[\"return\"].replace([np.NaN, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desanido las columnas del dataset \"credits.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"credits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazamos NaN por una lista compuesta por un diccionario vacío\n",
    "df2['cast'] = df2['cast'].apply(lambda x: [] if pd.isna(x) else x)\n",
    "\n",
    "# Utilizamos una función personalizada que verifique si el valor es una cadena y \n",
    "# luego intente convertirla en una lista para finalmente extraer los nombres del cast\n",
    "\n",
    "def extract_cast_names(value):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            cast_list = ast.literal_eval(value)\n",
    "            return [d.get('name') for d in cast_list]\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "    elif isinstance(value, list):\n",
    "        return [d.get('name') for d in value]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Creamos la columna \"cast_names\" compuesta por los valores de la clave\n",
    "#\"name\" de los diccionarios de las listas de la columna \"cast\"\n",
    "\n",
    "df2['cast_names'] = df2['cast'].apply(extract_cast_names)\n",
    "\n",
    "#Reemplazamos las listas vacías por None\n",
    "df2['cast_names'] = df2['cast_names'].replace([], None)\n",
    "\n",
    "#Eliminamos la columna \"cast\" original\n",
    "df2 = df2.drop('cast', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazamos NaN por una lista compuesta por un diccionario vacío\n",
    "df2['crew'] = df2['crew'].apply(lambda x: [] if pd.isna(x) else x)\n",
    "\n",
    "# Utilizamos una función personalizada que verifique si el valor es una cadena y \n",
    "#luego intente convertirla en una lista para finalmente extraer los nombres del crew\n",
    "\n",
    "def extract_crew_names(value):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            crew_list = ast.literal_eval(value)\n",
    "            return [d.get('name') for d in crew_list]\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "    elif isinstance(value, list):\n",
    "        return [d.get('name') for d in value]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Creamos la columna \"crew_names\" compuesta por los valores de la clave\n",
    "#\"name\" de los diccionarios de las listas de la columna \"crew\"\n",
    "\n",
    "df2['crew_names'] = df2['crew'].apply(extract_crew_names)\n",
    "\n",
    "#Reemplazamos las listas vacías por None\n",
    "df2['crew_names'] = df2['crew_names'].replace([], None)\n",
    "\n",
    "#Eliminamos la columna \"crew\" original\n",
    "df2 = df2.drop('crew', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quito uno de las 2 columnas \"id\" para luego unir ambos dataframes\n",
    "df2 = df2.drop(\"id\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uno ambos dataframes en uno para empezar a hacer las funciones\n",
    "df = pd.concat([df ,df2], axis= 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora construire las funciones que se requieren para hacer las peticiones a la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cantidad_filmaciones_mes(mes):\n",
    "    '''Determina la cantidad de películas que se estrenaron en el mes dado'''\n",
    "   \n",
    "    peliculas_mes = df['id'][df['release_month'] == mes].count()\n",
    "    return f\"{int(peliculas_mes)} es la cantidad de películas que fueron estrenadas en el mes de {mes}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cantidad_filmaciones_dia(dia):\n",
    "    '''Determina la cantidad de películas que se estrenaron en el día dado'''\n",
    "   \n",
    "    peliculas_dia = df['id'][df['release_day'] == dia].count()\n",
    "    return f\"{int(peliculas_dia)} es la cantidad de películas que fueron estrenadas en los días {dia}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_titulo(titulo_de_la_filmacion):\n",
    "    '''Se ingresa el título de una filmación esperando como respuesta el título, el año de estreno y el score.'''\n",
    "\n",
    "    titulo = titulo_de_la_filmacion\n",
    "    anio_estreno = df[\"release_year\"][df[\"title\"] == titulo_de_la_filmacion]\n",
    "    score = df[\"popularity\"][df[\"title\"] == titulo_de_la_filmacion]\n",
    "\n",
    "    return f\"La película {str(titulo)} fue estrenada en el año {int(anio_estreno)} con un score/popularidad de {float(score)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def votos_titulo(titulo_de_la_filmacion):\n",
    "    '''Se ingresa el título de una filmación esperando como respuesta el título, la cantidad de votos y el valor promedio de las votaciones. La misma variable deberá de contar con al menos 2000 valoraciones, caso contrario, debemos contar con un mensaje avisando que no cumple esta condición y que por ende, no se devuelve ningun valor.'''\n",
    "\n",
    "    titulo = titulo_de_la_filmacion\n",
    "    anio_estreno = int(df[\"release_year\"][df[\"title\"] == titulo_de_la_filmacion])\n",
    "    cantidad_votos = int(df[\"vote_count\"][df[\"title\"] == titulo_de_la_filmacion])\n",
    "    votos_promedio = float(df[\"vote_average\"][df[\"title\"] == titulo_de_la_filmacion])\n",
    "\n",
    "    if cantidad_votos < 2000:\n",
    "        return \"Esta peliculas no cumple con la valoracion necesaria para una opinion objetiva\"\n",
    "\n",
    "    return f\"La película {titulo} fue estrenada en el año {anio_estreno}. La misma cuenta con un total de {cantidad_votos} valoraciones, con un promedio de {votos_promedio}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(nombre_actor):\n",
    "    \n",
    "    '''Se ingresa el nombre de un actor que se encuentre dentro de un dataset debiendo devolver el éxito del mismo medido a través del retorno. Además, la cantidad de películas que en las que ha participado y el promedio de retorno. La definición no deberá considerar directores.'''\n",
    "\n",
    "    contador = 0\n",
    "    ganancia = 0\n",
    "    retorno = 0\n",
    "\n",
    "    for i in range(df[\"cast_names\"].size):\n",
    "        if nombre_actor in df[\"cast_names\"][i]:\n",
    "            ganancia += df.iloc[i, 10]\n",
    "            retorno += df.iloc[i, 20]\n",
    "            contador += 1\n",
    "            prom_retorno = retorno / contador\n",
    "\n",
    "\n",
    "    return f\"El actor {nombre_actor} ha participado de {contador} filmaciones, el mismo ha conseguido un retorno de {ganancia} con un promedio de {prom_retorno} por filmación\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_director(nombre_director): \n",
    "    '''Se ingresa el nombre de un director que se encuentre dentro de un dataset debiendo devolver el éxito del mismo medido a través del retorno. Además, deberá devolver el nombre de cada película con la fecha de lanzamiento, retorno individual, costo y ganancia de la misma.'''\n",
    "\n",
    "    retorno = 0\n",
    "\n",
    "    titulo_pelicula = \"\"\n",
    "    fecha_lanzamiento = \"\"\n",
    "    retorno_ind = 0\n",
    "    costo = 0\n",
    "    peliculas = []\n",
    "\n",
    "    for i in range(df[\"crew_names\"].size):\n",
    "        if nombre_director in df[\"crew_names\"][i]:\n",
    "            retorno += df.iloc[i, 10]\n",
    "\n",
    "            titulo_pelicula =  peliculas.append(df.iloc[i, 14])\n",
    "            fecha_lanzamiento =  peliculas.append(df.iloc[i, 9])\n",
    "            retorno_ind =  peliculas.append(df.iloc[i, 10])\n",
    "            costo = peliculas.append(df.iloc[i, 1])\n",
    "\n",
    "    return f\"Para el Director {nombre_director} el retorno a traves de su carrera fue de {retorno}, y aca informacion sobre sus peliculas: \\n {peliculas}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creacion del modelo de recomendacion con ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendacion(titulo):\n",
    "\n",
    "    num_recommendations=5\n",
    "    # Cargar los datos de películas\n",
    "    movies = pd.read_csv(\"data_con_EDA.csv\")\n",
    "\n",
    "    # Crear una matriz TF-IDF a partir de los títulos de las películas\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(movies[\"title\"].values.astype('U'))\n",
    "\n",
    "    # Entrenar un modelo de clustering (K-Means)\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "\n",
    "    # Encontrar el cluster más cercano al título de la película proporcionado\n",
    "    movie_vector = vectorizer.transform([titulo])\n",
    "    cluster_label = kmeans.predict(movie_vector)[0]\n",
    "\n",
    "    # Filtrar las películas en el mismo cluster que la película de interés\n",
    "    cluster_movies = movies[kmeans.labels_ == cluster_label]\n",
    "\n",
    "    # Excluir la película de interés de las recomendaciones\n",
    "    cluster_movies = cluster_movies[cluster_movies[\"title\"] != titulo]\n",
    "\n",
    "    # Ordenar las películas por su similitud al título de interés\n",
    "    cluster_movies[\"similitud\"] = kmeans.transform(vectorizer.transform(cluster_movies[\"title\"].values.astype('U'))).sum(axis=1)\n",
    "    cluster_movies = cluster_movies.sort_values(\"similitud\", ascending=False).head(num_recommendations)\n",
    "\n",
    "    # Devolver las películas recomendadas\n",
    "    recommendations = cluster_movies[\"title\"].tolist()\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_con_EDA.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
